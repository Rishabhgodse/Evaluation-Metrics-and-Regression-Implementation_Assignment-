{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyyMmtEAyS3T"
      },
      "outputs": [],
      "source": [
        "                                          #                 Theoretical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What does R-squared represent in a regression model?"
      ],
      "metadata": {
        "id": "dEn_CXmRyaUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R-squared, or the coefficient of determination, represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, where 1 indicates that the model perfectly explains the variance, and 0 indicates that the model explains none of the variance.\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "# Example data\n",
        "y_true = [3, -0.5, 2, 7]\n",
        "y_pred = [2.5, 0.0, 2, 8]\n",
        "r_squared = r2_score(y_true, y_pred)\n",
        "print(\"R-squared:\", r_squared)\n"
      ],
      "metadata": {
        "id": "XaBFfxPAyfcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the assumptions of linear regression?"
      ],
      "metadata": {
        "id": "0-YcVq4lyn7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "inear regression makes the following assumptions:\n",
        "\n",
        "Linearity: The relationship between the independent and dependent variable should be linear.\n",
        "Independence: Observations are independent of each other.\n",
        "Homoscedasticity: The residuals have constant variance.\n",
        "Normality of residuals: The residuals of the model should be normally distributed.\n",
        "No multicollinearity: Independent variables should not be highly correlated.\n",
        "\n",
        "Code to check assumptions:\n",
        "\n",
        "Check linearity using scatter plots.\n",
        "Check homoscedasticity with a residual vs. fitted plot.\n",
        "Check normality of residuals using a Q-Q plot.\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Residuals Normality Check\n",
        "residuals = np.random.normal(0, 1, 100)  # Dummy residuals\n",
        "sm.qqplot(residuals, line ='45')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S0_Rv9qNyuME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the difference between R-squared and Adjusted R-squared?"
      ],
      "metadata": {
        "id": "v7nKWwxRy5fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R-squared only considers the proportion of variance explained by the model.\n",
        "\n",
        "Adjusted R-squared adjusts for the number of predictors, penalizing the model for adding non-useful predictors.\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "X = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
        "y = [2, 3, 5, 7]\n",
        "X = sm.add_constant(X)\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "print(\"R-squared:\", model.rsquared)\n",
        "print(\"Adjusted R-squared:\", model.rsquared_adj)\n"
      ],
      "metadata": {
        "id": "BqR-GfxjzGio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Why do we use Mean Squared Error (MSE)?"
      ],
      "metadata": {
        "id": "aouuDOCBzODB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE is used to measure the average of the squares of the errors. The squaring of errors gives more weight to larger errors, helping to penalize models that make significant mistakes.\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "y_true = [3, -0.5, 2, 7]\n",
        "y_pred = [2.5, 0.0, 2, 8]\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n"
      ],
      "metadata": {
        "id": "bdimHLuezTm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What does an Adjusted R-squared value of 0.85 indicate?"
      ],
      "metadata": {
        "id": "mWcUuSeFzhMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An Adjusted R-squared value of 0.85 means that 85% of the variance in the dependent variable is explained by the model, accounting for the number of predictors. This shows a strong fit.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Sample dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 5)  # 100 samples and 5 features\n",
        "y = 3 * X[:, 0] + 2 * X[:, 1] + np.random.randn(100)  # Target variable with some noise\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Add a constant to the input data\n",
        "X_train_sm = sm.add_constant(X_train)\n",
        "\n",
        "# Fit the OLS (Ordinary Least Squares) model\n",
        "ols_model = sm.OLS(y_train, X_train_sm).fit()\n",
        "\n",
        "# Print R-squared and Adjusted R-squared\n",
        "print(\"R-squared:\", ols_model.rsquared)\n",
        "print(\"Adjusted R-squared:\", ols_model.rsquared_adj)\n",
        "# Add a constant to the input data\n",
        "X_train_sm = sm.add_constant(X_train)\n",
        "\n",
        "# Fit the OLS (Ordinary Least Squares) model\n",
        "ols_model = sm.OLS(y_train, X_train_sm).fit()\n",
        "\n",
        "# Print R-squared and Adjusted R-squared\n",
        "print(\"R-squared:\", ols_model.rsquared)\n",
        "print(\"Adjusted R-squared:\", ols_model.rsquared_adj)\n"
      ],
      "metadata": {
        "id": "xkF7zfExzoh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How do we check for normality of residuals in linear regression?"
      ],
      "metadata": {
        "id": "U3e_Wy2Q0Jjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check normality by plotting the residuals and using statistical tests.\n",
        "# Q-Q plot for normality\n",
        "sm.qqplot(residuals, line='45')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8R-W50M30QQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is multicollinearity, and how does it impact regression?"
      ],
      "metadata": {
        "id": "YiLZvftR0Ytf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multicollinearity occurs when two or more independent variables are highly correlated. It can make the regression coefficients unstable and difficult to interpret.\n",
        "\n",
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Assume df is a DataFrame with your predictor variables\n",
        "X = pd.DataFrame([[1, 2], [2, 3], [3, 4], [4, 5]])\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "4vwFzHPU0wGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is Mean Absolute Error (MAE)?"
      ],
      "metadata": {
        "id": "Cpk_eZLe05JD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAE is the average of the absolute differences between actual and predicted values. Unlike MSE, it does not penalize larger errors more than smaller ones.\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "print(\"Mean Absolute Error:\", mae)\n"
      ],
      "metadata": {
        "id": "PmV223Mk09_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What are the benefits of using an ML pipeline?"
      ],
      "metadata": {
        "id": "8-Zau4uj1IqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML pipelines automate and streamline workflows, improving consistency and reproducibility. Pipelines allow the combination of data preprocessing steps with model training and evaluation.\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "pipeline = Pipeline([('scaler', StandardScaler()), ('model', LinearRegression())])\n",
        "pipeline.fit(X, y)\n"
      ],
      "metadata": {
        "id": "T_G54Qtw1cbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Why is RMSE considered more interpretable than MSE?"
      ],
      "metadata": {
        "id": "0Ju5S4XL1jaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE is more interpretable because it is in the same units as the dependent variable, making it easier to understand in the context of the original data.\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "print(\"Root Mean Squared Error:\", rmse)\n"
      ],
      "metadata": {
        "id": "7GJXTfgd1pQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is pickling in Python, and how is it useful in ML?"
      ],
      "metadata": {
        "id": "-B7Imusu10Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pickling is the process of serializing a Python object. It is useful in ML for saving trained models to be reused later without retraining.\n",
        "import pickle\n",
        "\n",
        "# Saving the model\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(pipeline, f)\n",
        "\n",
        "# Loading the model\n",
        "with open('model.pkl', 'rb') as f:\n",
        "    loaded_model = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "P1DpzVNn188K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What does a high R-squared value mean?"
      ],
      "metadata": {
        "id": "UMjwvUOA2IVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A high R-squared value indicates that a large proportion of the variance in the dependent variable (target) is explained by the independent variables (features). A value closer to 1 means the model fits the data well, but it doesn't necessarily mean the model is perfect. It may also be a sign of overfitting.\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Sample data\n",
        "X = [[1], [2], [3], [4]]\n",
        "y = [1, 2, 3, 4]\n",
        "\n",
        "# Linear Regression\n",
        "model = LinearRegression().fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "r_squared = r2_score(y, y_pred)\n",
        "\n",
        "print(f\"R-squared: {r_squared}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Vslug6nR3ACL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What happens if linear regression assumptions are violated?"
      ],
      "metadata": {
        "id": "a-GJRFty3I4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Violation of Linearity: The model will fail to capture the true relationship between variables, leading to inaccurate predictions.\n",
        "\n",
        "Violation of Homoscedasticity: Results in inefficient estimates and biased inferences.\n",
        "\n",
        "Violation of Normality of Residuals: Can lead to incorrect confidence intervals and p-values.\n",
        "\n",
        "Violation of No Multicollinearity: Can make it difficult to identify the individual effect of predictors.\n",
        "\n",
        "Violation of Independence: Can lead to over- or underestimation of the significance of variables.\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Residual Plot to check Homoscedasticity\n",
        "residuals = np.random.normal(0, 1, 100)  # Dummy residuals\n",
        "plt.scatter(range(100), residuals)\n",
        "plt.xlabel(\"Fitted values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residual Plot to Check Homoscedasticity\")\n",
        "plt.show()\n",
        "\n",
        "# Q-Q plot to check Normality\n",
        "sm.qqplot(residuals, line='45')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OS6wtitd3Ooo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How can we address multicollinearity in regression?"
      ],
      "metadata": {
        "id": "h55VrvHS3ZZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multicollinearity occurs when independent variables are highly correlated. It can be addressed by:\n",
        "\n",
        "Removing correlated variables: Dropping one of the correlated variables.\n",
        "Using Principal Component Analysis (PCA): To reduce dimensionality.\n",
        "Ridge or Lasso Regression: Regularization methods that penalize multicollinearity.\n",
        "\n",
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Sample Data\n",
        "X = pd.DataFrame({\n",
        "    'feature_1': [1, 2, 3, 4, 5],\n",
        "    'feature_2': [2, 4, 6, 8, 10],  # Highly correlated with feature_1\n",
        "    'feature_3': [1, 3, 5, 7, 9]\n",
        "})\n",
        "\n",
        "# Calculating VIF\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "vif_data[\"Feature\"] = X.columns\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "cuH1KVFw3jpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How can feature selection improve model performance in regression analysis?"
      ],
      "metadata": {
        "id": "qi3iobIc3swo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection removes irrelevant or redundant features, which can:\n",
        "\n",
        "Reduce overfitting.\n",
        "\n",
        "Improve model interpretability.\n",
        "\n",
        "Speed up computation.\n",
        "\n",
        "Avoid multicollinearity.\n",
        "\n",
        "Methods of feature selection include Recursive Feature Elimination (RFE) and Regularization techniques like Lasso Regression.\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample Data\n",
        "X = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]\n",
        "y = [1, 2, 3, 4]\n",
        "\n",
        "# Create a model and apply RFE\n",
        "model = LinearRegression()\n",
        "selector = RFE(model, n_features_to_select=2)\n",
        "selector = selector.fit(X, y)\n",
        "\n",
        "print(\"Selected Features:\", selector.support_)\n",
        "print(\"Feature Ranking:\", selector.ranking_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rG9Q17iP3zZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How is Adjusted R-squared calculated?"
      ],
      "metadata": {
        "id": "L_J5LsjK4XTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjusted R-squared is a modification of R-squared that adjusts for the number of predictors in the model. It is calculated as:\n",
        "\n",
        "Adjusted\n",
        "ùëÖ\n",
        "2\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "(\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùëÖ\n",
        "2\n",
        ")\n",
        "(\n",
        "ùëõ\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "ùëõ\n",
        "‚àí\n",
        "ùëù\n",
        "‚àí\n",
        "1\n",
        ")\n",
        "Adjusted¬†R\n",
        "2\n",
        " =1‚àí(\n",
        "n‚àíp‚àí1\n",
        "(1‚àíR\n",
        "2\n",
        " )(n‚àí1)\n",
        "‚Äã\n",
        " )\n",
        "Where:\n",
        "\n",
        "ùëõ\n",
        "n is the number of data points.\n",
        "ùëù\n",
        "p is the number of predictors.\n",
        "\n",
        "import statsmodels.api as sm\n",
        "\n",
        "X = sm.add_constant(X)  # Adding constant for intercept\n",
        "model = sm.OLS(y, X).fit()\n",
        "print(f\"R-squared: {model.rsquared}\")\n",
        "print(f\"Adjusted R-squared: {model.rsquared_adj}\")\n"
      ],
      "metadata": {
        "id": "0_ChqwmH4eYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Why is MSE sensitive to outliers?"
      ],
      "metadata": {
        "id": "TRJMGei_4lXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mean Squared Error (MSE) squares the error values, which means larger errors (such as those caused by outliers) have a disproportionately high impact. This can skew the error metric, making it sensitive to outliers.\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "y_true = [3, -0.5, 2, 7, 10]\n",
        "y_pred = [2.5, 0.0, 2, 8, 50]  # Last value is an outlier\n",
        "\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "print(\"MSE with Outliers:\", mse)\n"
      ],
      "metadata": {
        "id": "PCqowqi64swt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the role of homoscedasticity in linear regression?"
      ],
      "metadata": {
        "id": "5bFfhCpk4zaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homoscedasticity means that the variance of the residuals is constant across all levels of the independent variables. Violating this assumption (heteroscedasticity) can result in inefficient estimates, affecting the reliability of the model‚Äôs coefficients and predictions.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "residuals = y_true - y_pred\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs Predicted\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yDsgPntU5CxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is Root Mean Squared Error (RMSE)?"
      ],
      "metadata": {
        "id": "iNGpaOuS5M2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE is the square root of the Mean Squared Error (MSE), which makes it easier to interpret because it is in the same units as the dependent variable. It provides a good measure of how well the model‚Äôs predictions match the actual data.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "print(\"RMSE:\", rmse)\n"
      ],
      "metadata": {
        "id": "SpOJn20l5RZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Why is pickling considered risky?"
      ],
      "metadata": {
        "id": "BuV9biMj5aRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pickling can be risky because it allows for arbitrary code execution when loading the serialized object. If the pickle file is maliciously altered, it can lead to a security vulnerability by executing harmful code during deserialization."
      ],
      "metadata": {
        "id": "NSGsZQSD5eyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What alternatives exist to pickling for saving ML models?"
      ],
      "metadata": {
        "id": "4cUREAuX5kzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Safer alternatives to pickling include:\n",
        "\n",
        "Joblib: Efficient for large models or arrays.\n",
        "HDF5 (via TensorFlow/Keras): Commonly used for deep learning models.\n",
        "ONNX (Open Neural Network Exchange): Cross-platform, format-agnostic approach for saving models.\n",
        "\n",
        "\n",
        "import joblib\n",
        "\n",
        "# Saving the model\n",
        "joblib.dump(model, 'model.joblib')\n",
        "\n",
        "# Loading the model\n",
        "loaded_model = joblib.load('model.joblib')\n"
      ],
      "metadata": {
        "id": "sfTpj5Yc5t0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is heteroscedasticity, and why is it a problem?"
      ],
      "metadata": {
        "id": "NnDew-lJ5za1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heteroscedasticity occurs when the variance of residuals is not constant. This violates one of the key assumptions of linear regression, leading to inefficient estimates and biased standard errors, which can affect the validity of hypothesis tests.\n",
        "\n",
        "from statsmodels.stats.diagnostic import het_breuschpagan\n",
        "\n",
        "# Performing Breusch-Pagan test\n",
        "bp_test = het_breuschpagan(residuals, X)\n",
        "print(f\"Breusch-Pagan Test p-value: {bp_test[1]}\")\n"
      ],
      "metadata": {
        "id": "6qzU72Zo55Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How can interaction terms enhance a regression model's predictive power?"
      ],
      "metadata": {
        "id": "85G5N8JX59_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interaction terms allow the model to capture the combined effect of two or more variables. In many cases, the impact of one feature on the dependent variable depends on the value of another feature. Adding interaction terms can improve the model's predictive power by incorporating these joint effects.\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Adding interaction terms\n",
        "poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
        "X_interaction = poly.fit_transform(X)\n",
        "\n",
        "# Fit the model with interaction terms\n",
        "model_interaction = LinearRegression().fit(X_interaction, y)\n"
      ],
      "metadata": {
        "id": "JjcdwQZL6CWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                                                      #          Practical"
      ],
      "metadata": {
        "id": "kJwzKSrd6VX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python script to visualize the distribution of errors (residuals) for a multiple linear regression model using Seaborn's \"diamonds\" dataset."
      ],
      "metadata": {
        "id": "UV-s7slV7LcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Select features and target variable\n",
        "X = diamonds[['carat', 'depth', 'table']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate residuals\n",
        "y_pred = model.predict(X_test)\n",
        "residuals = y_test - y_pred\n",
        "\n",
        "# Plot the distribution of residuals\n",
        "sns.histplot(residuals, kde=True)\n",
        "plt.xlabel('Residuals')\n",
        "plt.title('Distribution of Residuals')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ajKlLtLX7RDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python script to calculate and print Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) for a linear regression model."
      ],
      "metadata": {
        "id": "wSpuZU_e7Szm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import numpy as np\n",
        "\n",
        "# Calculate MSE, MAE, RMSE\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"RMSE: {rmse}\")\n"
      ],
      "metadata": {
        "id": "x-6BCTrL7b50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python script to check if the assumptions of linear regression are met. Use a scatter plot to check linearity, residuals plot for homoscedasticity, and correlation matrix for multicollinearity."
      ],
      "metadata": {
        "id": "GdrWup2z7iWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Linearity (scatter plot)\n",
        "sns.pairplot(diamonds[['carat', 'depth', 'table', 'price']])\n",
        "plt.show()\n",
        "\n",
        "# Homoscedasticity (residuals plot)\n",
        "sns.residplot(x=y_pred, y=residuals, lowess=True)\n",
        "plt.xlabel(\"Fitted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals vs Fitted\")\n",
        "plt.show()\n",
        "\n",
        "# Multicollinearity (correlation matrix and VIF)\n",
        "corr_matrix = X_train.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
        "plt.show()\n",
        "\n",
        "# VIF\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_train.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "kzbaxGcQ7m4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python script that creates a machine learning pipeline with feature scaling and evaluates the performance of different regression models."
      ],
      "metadata": {
        "id": "Snb1vUZB7pOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regression', Ridge())\n",
        "])\n",
        "\n",
        "# Evaluate model performance using cross-validation\n",
        "scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='r2')\n",
        "print(\"Cross-validated R-squared scores:\", scores)\n",
        "print(\"Mean R-squared:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "aKQ7-6EX784v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Implement a simple linear regression model on a dataset and print the model's coefficients, intercept, and R-squared score."
      ],
      "metadata": {
        "id": "Vnc6db6O7_7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train simple linear regression\n",
        "simple_model = LinearRegression()\n",
        "simple_model.fit(X_train[['carat']], y_train)\n",
        "\n",
        "# Print coefficients, intercept, and R-squared score\n",
        "print(f\"Coefficient: {simple_model.coef_[0]}\")\n",
        "print(f\"Intercept: {simple_model.intercept_}\")\n",
        "print(f\"R-squared: {simple_model.score(X_test[['carat']], y_test)}\")\n"
      ],
      "metadata": {
        "id": "6qrbhCDD8FGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python script that analyzes the relationship between total bill and tip in the 'tips' dataset using simple linear regression and visualizes the results."
      ],
      "metadata": {
        "id": "N4ohJD5C8Iyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Define X and y\n",
        "X = tips[['total_bill']]\n",
        "y = tips['tip']\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Plot regression line\n",
        "sns.regplot(x='total_bill', y='tip', data=tips, line_kws={\"color\": \"red\"})\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bSo-wl6Q8ac_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python script that fits a linear regression model to a synthetic dataset with one feature. Use the model to predict new values and plot the data points along with the regression line."
      ],
      "metadata": {
        "id": "0KUxCYp68cla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 2.5 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot data and regression line\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mXTiLuwF8hU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python script that pickles a trained linear regression model and saves it to a file."
      ],
      "metadata": {
        "id": "KuMHz5QO8kPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import pickle\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save model to file\n",
        "with open('linear_regression_model.pkl', 'wb') as file:\n",
        "    pickle.dump(model, file)\n",
        "\n",
        "# Load model from file\n",
        "with open('linear_regression_model.pkl', 'rb') as file:\n",
        "    loaded_model = pickle.load(file)\n"
      ],
      "metadata": {
        "id": "zs71BCAG8oh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python script that fits a polynomial regression model (degree 2) to a dataset and plots the regression curve."
      ],
      "metadata": {
        "id": "zd-GhUoo8sLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Polynomial transformation\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Train polynomial regression model\n",
        "model_poly = LinearRegression()\n",
        "model_poly.fit(X_poly, y)\n",
        "\n",
        "# Plot polynomial regression curve\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, model_poly.predict(X_poly), color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Vsz2d85OAYB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data for simple linear regression (use random values for X and y) and fit a linear regression model to the data. Print the model's coefficient and intercept"
      ],
      "metadata": {
        "id": "JPY6tZysAZs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print coefficient and intercept\n",
        "print(f\"Coefficient: {model.coef_[0][0]}\")\n",
        "print(f\"Intercept: {model.intercept_[0]}\")\n"
      ],
      "metadata": {
        "id": "pkett7i4AdTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python script that fits polynomial regression models of different degrees to a synthetic dataset and compares their performance"
      ],
      "metadata": {
        "id": "Rc9OPSs5AgH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "degrees = [1, 2, 3]\n",
        "for degree in degrees:\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "    score = model.score(X_poly, y)\n",
        "    print(f\"Degree {degree}: R-squared = {score}\")\n"
      ],
      "metadata": {
        "id": "PI6e5Aw7Ajgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# . Write a Python script that fits a simple linear regression model with two features and prints the model's coefficients, intercept, and R-squared score."
      ],
      "metadata": {
        "id": "9e-NagOcAlit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select two features\n",
        "X = diamonds[['carat', 'depth']]\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print coefficients, intercept, and R-squared score\n",
        "print(f\"Coefficients: {model.coef_}\")\n",
        "print(f\"Intercept: {model.intercept_}\")\n",
        "print(f\"R-squared: {model.score(X, y)}\")\n"
      ],
      "metadata": {
        "id": "PnBsPk-LApMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the regression line along with the data points."
      ],
      "metadata": {
        "id": "Y7_AWegmAtOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 3 + 2 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot data and regression line\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O0Vh8MciAwWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset with multiple features."
      ],
      "metadata": {
        "id": "6leeey20Ay-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "# Load dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Select relevant features\n",
        "X = diamonds[['carat', 'depth', 'table']]\n",
        "\n",
        "# Add constant column for VIF calculation\n",
        "X_const = add_constant(X)\n",
        "\n",
        "# Calculate VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X_const.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]\n",
        "\n",
        "print(vif_data)\n"
      ],
      "metadata": {
        "id": "XcwT7GLUGkTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a polynomial regression model, and plots the regression curve."
      ],
      "metadata": {
        "id": "HQQn6BrKGmZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = 3 * X**4 - 2 * X**3 + X**2 + 5 + np.random.randn(100, 1)\n",
        "\n",
        "# Polynomial transformation (degree 4)\n",
        "poly = PolynomialFeatures(degree=4)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Plot the regression curve\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, model.predict(X_poly), color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ejvkfrSrGroE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python script that creates a machine learning pipeline with data standardization and a multiple linear regression model, and prints the R-squared score."
      ],
      "metadata": {
        "id": "ntpQqjdfGuTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Select features and target\n",
        "X = diamonds[['carat', 'depth', 'table']]\n",
        "y = diamonds['price']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('regression', LinearRegression())\n",
        "])\n",
        "\n",
        "# Fit the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate the model\n",
        "y_pred = pipeline.predict(X_test)\n",
        "print(f\"R-squared: {r2_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "pB6VsDUZGzAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the regression curve."
      ],
      "metadata": {
        "id": "IkL_2TlPG2dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate synthetic data\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = 2 * X**3 - X**2 + 4 + np.random.randn(100, 1)\n",
        "\n",
        "# Polynomial transformation (degree 3)\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Fit polynomial regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Plot the regression curve\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, model.predict(X_poly), color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YvZ2ZpzGG7Oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print the R-squared score and model coefficients."
      ],
      "metadata": {
        "id": "wHmTWA6SG-Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 5)\n",
        "y = 2 + 3 * X[:, 0] - 4 * X[:, 1] + 5 * X[:, 2] - X[:, 3] + 2 * X[:, 4] + np.random.randn(100)\n",
        "\n",
        "# Train multiple linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print R-squared and coefficients\n",
        "print(f\"R-squared: {model.score(X, y)}\")\n",
        "print(f\"Coefficients: {model.coef_}\")\n"
      ],
      "metadata": {
        "id": "YKtOudYIHBmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python script that generates synthetic data for linear regression, fits a model, and visualizes the data points along with the regression line."
      ],
      "metadata": {
        "id": "sBxWheFFHE4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 1) * 10\n",
        "y = 3 + 2 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict values\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Plot data and regression line\n",
        "plt.scatter(X, y, color='blue')\n",
        "plt.plot(X, y_pred, color='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uBbHZ_ToHIaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's Rsquared score and coefficients."
      ],
      "metadata": {
        "id": "AN4GsF0KHLsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 3)\n",
        "y = 5 + 2 * X[:, 0] - 3 * X[:, 1] + X[:, 2] + np.random.randn(100)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Print R-squared and coefficients\n",
        "print(f\"R-squared: {model.score(X, y)}\")\n",
        "print(f\"Coefficients: {model.coef_}\")\n"
      ],
      "metadata": {
        "id": "aF9JwHLTHOpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python script that demonstrates how to serialize and deserialize machine learning models using joblib instead of pickling."
      ],
      "metadata": {
        "id": "-cmYrGoNHRl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import joblib\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save model using joblib\n",
        "joblib.dump(model, 'linear_regression_model.joblib')\n",
        "\n",
        "# Load model using joblib\n",
        "loaded_model = joblib.load('linear_regression_model.joblib')\n"
      ],
      "metadata": {
        "id": "7bQMyKSzHV0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Write a Python script to perform linear regression with categorical features using one-hot encoding. Use the Seaborn 'tips' dataset."
      ],
      "metadata": {
        "id": "OFvtg6U5HYwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the 'tips' dataset\n",
        "tips = sns.load_dataset('tips')\n",
        "\n",
        "# Define features and target\n",
        "X = tips[['sex', 'day', 'time']]\n",
        "y = tips['total_bill']\n",
        "\n",
        "# Create a pipeline with one-hot encoding for categorical variables\n",
        "pipeline = Pipeline([\n",
        "    ('encoder', ColumnTransformer([\n",
        "        ('onehot', OneHotEncoder(), ['sex', 'day', 'time'])\n",
        "    ])),\n",
        "    ('regression', LinearRegression())\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X, y)\n",
        "\n",
        "# Predict and print R-squared\n",
        "y_pred = pipeline.predict(X)\n",
        "print(f\"R-squared: {pipeline.score(X, y)}\")\n"
      ],
      "metadata": {
        "id": "xLYpAEUJHcWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and Rsquared score."
      ],
      "metadata": {
        "id": "2Ga8DjWKHfSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 3)\n",
        "y = 2 + 3 * X[:, 0] - 2 * X[:, 1] + X[:, 2] + np.random.randn(100)\n",
        "\n",
        "# Train Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X, y)\n",
        "\n",
        "# Train Ridge Regression model\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "ridge_model.fit(X, y)\n",
        "\n",
        "# Print coefficients and R-squared for both models\n",
        "print(\"Linear Regression Coefficients:\", linear_model.coef_)\n",
        "print(\"Linear Regression R-squared:\", linear_model.score(X, y))\n",
        "\n",
        "print(\"Ridge Regression Coefficients:\", ridge_model.coef_)\n",
        "print(\"Ridge Regression R-squared:\", ridge_model.score(X, y))\n"
      ],
      "metadata": {
        "id": "6JSa9OMqHjPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic dataset."
      ],
      "metadata": {
        "id": "xylosqwuHmVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.random.rand(100, 3)\n",
        "y = 2 + 3 * X[:, 0] - 2 * X[:, 1] + X[:, 2] + np.random.randn(100)\n",
        "\n",
        "# Create model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Evaluate using cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
        "print(\"Cross-validated R-squared scores:\", scores)\n",
        "print(\"Mean R-squared:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "YUAKdko2HtZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a Python script that compares polynomial regression models of different degrees and prints the Rsquared score for each."
      ],
      "metadata": {
        "id": "S_-BP-StHy9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate synthetic data\n",
        "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
        "y = 2 * X**3 - X**2 + 4 + np.random.randn(100, 1)\n",
        "\n",
        "# Compare polynomial regression models of degrees 1, 2, 3, 4\n",
        "for degree in range(1, 5):\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X)\n",
        "    \n",
        "    # Train model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "    \n",
        "    # Print R-squared score\n",
        "    print(f\"Degree {degree}: R-squared = {model.score(X_poly, y)}\")\n"
      ],
      "metadata": {
        "id": "7prqDWBwH22w"
      }
    }
  ]
}